{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IshOqZNB6POh"
   },
   "source": [
    "\n",
    "**MLS Case Study: Twitter US Airline Sentiment - Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4Eouxvz5Rb3"
   },
   "source": [
    "## **Context:**\n",
    "\n",
    "- **This dataset consists of tweet and retweet data about the major US Airlines probelms.**\n",
    "- **The data spans 2015 and covers 14,640 reviews. Tweets include a plain text reasoning of the feedback.**\n",
    "- **It alo includes feeedbacks for the major US Airlines.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaP-CPyF5Ndl"
   },
   "source": [
    "#### The purpose of this analysis is to explore the numerous features and build a classification model where we will be able to tag the rating based on the description and consequently rate them as positive or negative. In this analysis, we will be focusing on score, summary, description and score based sentiment features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTBGvcc1XoE7"
   },
   "source": [
    "**Dataset:**\n",
    "\n",
    "The dataset has the following columns:\n",
    "\n",
    "* tweet_id\n",
    "* airline_sentiment\n",
    "* airline_sentiment_confidence\n",
    "* negativereason\n",
    "* negativereason_confidence\n",
    "* airline\n",
    "* airline_sentiment_gold\n",
    "* name\n",
    "* negativereason_gold\n",
    "* retweet_count\n",
    "* text\n",
    "* tweet_coord\n",
    "* tweet_created\n",
    "* tweet_location\n",
    "* user_timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn7opYSoiNyK"
   },
   "source": [
    "##**Steps:**\n",
    "- Import the necessary libraries\n",
    "- Get the data\n",
    "- Explore the data\n",
    "- Do feature engineering (create relevant columns based on existing columns)\n",
    "- Plot the wordcloud based on the relevant column\n",
    "- Do pre-processing\n",
    "- Noise removal (Special character, html tags, numbers, \n",
    "stopword removal)\n",
    "- Lowercasing\n",
    "- Stemming / lemmatization\n",
    "- Text to number: Vectorization\n",
    "- CountVectorizer\n",
    "- TfidfVectorizer\n",
    "- Build Machine Learning Model for Text Classification.\n",
    "- Optimize the parameters to improve the\n",
    "accuracy\n",
    "- Plot the worldcloud based on the most important features\n",
    "- Check the performance of the model\n",
    "- Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3WGiRNpTvdE"
   },
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjnXaaLV5Qff"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries.\n",
    "\n",
    "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
    "import contractions                                     # Import contractions library.\n",
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "\n",
    "import numpy as np                                      # Import numpy.\n",
    "import pandas as pd                                     # Import pandas.\n",
    "import nltk                                             # Import Natural Language Tool-Kit.\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
    "import matplotlib.pyplot as plt                         # Import plt for visualization\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEflrSRHlun6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PT3Eepy6l7-t"
   },
   "outputs": [],
   "source": [
    "# read Tweets file from Google Drive\n",
    "import pandas as pd\n",
    "data=pd.read_csv('/content/drive/MyDrive/colab/Project - Natural Language Processing/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAxBWyO29Qb6"
   },
   "outputs": [],
   "source": [
    "# check the data shape\n",
    "data.shape                                               # print shape of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO4qGyNly_QF"
   },
   "source": [
    "- Tweets file contains 14,640 records and 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAJWb__77ZNu"
   },
   "outputs": [],
   "source": [
    "# checking sample data\n",
    "pd.set_option('display.max_colwidth', None) # Display full dataframe information (Non-turncated Text column.)\n",
    "data.head()                                              # Print first 5 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjE1cHUWYNSg"
   },
   "outputs": [],
   "source": [
    "# check data \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XGpxXGYZpL5"
   },
   "outputs": [],
   "source": [
    "# check data types of each column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTGoUkxAaEcu"
   },
   "outputs": [],
   "source": [
    "# convert object columns to category columns\n",
    "data[data.select_dtypes(['object']).columns] = data.select_dtypes(['object']).apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGC-ElQ2aIR7"
   },
   "outputs": [],
   "source": [
    "# check conversion\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM7kFZJRkh_7"
   },
   "outputs": [],
   "source": [
    "#Dropping tweet_id column from the dataframe as there no usage\n",
    "data.drop(columns=['tweet_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXNo_lihcyW4"
   },
   "outputs": [],
   "source": [
    "# data category list\n",
    "data.select_dtypes(['category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-a3CEi8gVBr"
   },
   "outputs": [],
   "source": [
    "# check values of the category columns\n",
    "cat_cols=data.select_dtypes(['category']).columns\n",
    "\n",
    "for column in cat_cols:\n",
    "    print(data[column].value_counts())\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fi1xVk_0g1v"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum(axis=0)                                # Check for NULL values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ho8Bx3FY-mL"
   },
   "source": [
    "**Categorical data analysis**\n",
    "\n",
    "* airline_sentiment has three distinct values: negative, positive and neutral\n",
    "* tweets data is avaialable for 5 airlines: United, US Airways, American, Southwest, Delta, Virgin America\n",
    "* airline_sentiment_gold, negativereason_gold, tweet_coord columns may not be useful for analysis.\n",
    "* One main negativereason given is \"Customer Service Issue\". However, there is not enough data in this column.\n",
    "* Following columns don't have meaningful data: name\n",
    "* Following columns don't have enough data: airline_sentiment_gold, negativereason_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y65o3mFNvlcA"
   },
   "outputs": [],
   "source": [
    "# define function to create labeled barplots to see individual columns data distribution\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SGYc8i1wEdP"
   },
   "outputs": [],
   "source": [
    "# let's check tweets by airlines\n",
    "labeled_barplot(data, \"airline\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAojVRxY0NQY"
   },
   "source": [
    "- 26% tweets are for United Airlines which is the biggest bucket\n",
    "- Virgin America has least number of tweeets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lvdk14N8vKG"
   },
   "outputs": [],
   "source": [
    "# let's look at the overall tweets sentimets \n",
    "labeled_barplot(data, \"airline_sentiment\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhlHyGoi0s1X"
   },
   "source": [
    "- Maximum tweets (62.7%) reflects negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc7WdtSO9pXm"
   },
   "outputs": [],
   "source": [
    "# let's explore distribution of negative sentiments\n",
    "labeled_barplot(data, \"negativereason\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_3puKhJ0-3N"
   },
   "source": [
    "- Almost 20% of the negative sentiments resulted from Customer Service Issues. This could be addressed with low investment.\n",
    "-  Next bucket is from Late Flights of 11.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L4r6veByg_9"
   },
   "outputs": [],
   "source": [
    "# defining function to plot the predictor and target variables\n",
    "def stacked_barplot(data, predictor, target):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    data: dataframe\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = data[predictor].nunique()\n",
    "    sorter = data[target].value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    print(tab1)\n",
    "    print(\"-\" * 120)\n",
    "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
    "    plt.legend(\n",
    "        loc=\"lower left\", frameon=False,\n",
    "    )\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MT_NtUqyykIN"
   },
   "outputs": [],
   "source": [
    "# plot the airline_sentiment by airline\n",
    "stacked_barplot(data, \"airline\", \"airline_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_FgX-u92xAv"
   },
   "source": [
    "- United has the maximum feeback avaialable\n",
    "- Virgin America has least tweets also lowest level of negative sentiment\n",
    "-  US Airways has the maximum negative feedbak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmOmaOaXCDaG"
   },
   "source": [
    "### **Word Cloud based on Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTUN1rVmCRTi"
   },
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtfvJP-3E_Kr"
   },
   "outputs": [],
   "source": [
    "# build the wordcloud to see the passenger sentiment\n",
    "import wordcloud\n",
    "def show_wordcloud(data, title):\n",
    "    text = ' '.join(data['text'].astype(str).tolist())                 # Converting Summary column into list\n",
    "    stopwords = set(wordcloud.STOPWORDS)                                  # instantiate the stopwords from wordcloud\n",
    "    \n",
    "    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='white',          # Setting the different parameter of stopwords\n",
    "                    colormap='viridis', width=800, height=600).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(14,11), frameon=True)                             \n",
    "    plt.imshow(fig_wordcloud)  \n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMWTvDSuFHpG"
   },
   "outputs": [],
   "source": [
    "# Let's look at the Negative sentiments visually\n",
    "show_wordcloud(data[data.airline_sentiment == \"negative\"], title = \"Negative Sentiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8ybMkc6GL2x"
   },
   "source": [
    "- Majority complains are for United, US Airways, AmericanAir, United and Southwest.\n",
    "- Customer Service, Flight Delay, Cancellation, Baggage Probelms are highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLcFI0m6FAyA"
   },
   "outputs": [],
   "source": [
    "show_wordcloud(data[data.airline_sentiment == \"positive\"], title = \"Positive Sentiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5f5ewDDGSZy"
   },
   "source": [
    "- JetBlue, SouthwestAir, AmericalAir shows up in the positive sentiment.\n",
    "- 'Thanks', 'Love', 'Appreciate', 'Amazing', 'Awesome' are highligted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiP4P38vOlFA"
   },
   "outputs": [],
   "source": [
    "# Only keeping relevant columns from the data, as these are useful for our analysis.\n",
    "data_sel = data.loc[:,['airline_sentiment','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtWdbYiL79M6"
   },
   "outputs": [],
   "source": [
    "data_sel.shape                                # Shape of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0tavxH_7s1f"
   },
   "source": [
    "- Only two columns - airline sentiment and tweet text is retained for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tmk3MTUfaab"
   },
   "outputs": [],
   "source": [
    "# checking the data types\n",
    "data_sel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "377IfFic-Ihk"
   },
   "source": [
    "### **Text Pre-processing:**\n",
    "\n",
    "- Remove html tags.\n",
    "- Replace contractions in string. (e.g. replace I'm --> I am) and so on.\\\n",
    "- Remove numbers.\n",
    "- Tokenization\n",
    "- To remove Stopwords.\n",
    "- Lemmatized data\n",
    "\n",
    "We have used the **NLTK library to tokenize words, remove stopwords and lemmatize the remaining words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf3kMpGN7Lp9"
   },
   "outputs": [],
   "source": [
    "# define function to remove html tags\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")                    \n",
    "    return soup.get_text()\n",
    "\n",
    "data_sel['text'] = data_sel['text'].apply(lambda x: strip_html(x))\n",
    "\n",
    "data_sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hafYVjdY8KyH"
   },
   "source": [
    "- no html tag is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7NgnYq77Puy"
   },
   "outputs": [],
   "source": [
    "# define function to remove contractions in the text\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data_sel['text'] = data_sel['text'].apply(lambda x: replace_contractions(x))\n",
    "\n",
    "data_sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL_FBXGV8chq"
   },
   "source": [
    "- no contraction string is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ga8ArYYxAB_1"
   },
   "outputs": [],
   "source": [
    "# define function to remove numbers in the text\n",
    "def remove_numbers(text):\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "  return text\n",
    "\n",
    "data_sel['text'] = data_sel['text'].apply(lambda x: remove_numbers(x))\n",
    "\n",
    "data_sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5q4cm_T8sUC"
   },
   "source": [
    "- sample data checking looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0-yYsx68DxT"
   },
   "outputs": [],
   "source": [
    "# tokenize the tweets texts for further analysis\n",
    "data_sel['text'] = data_sel.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) # Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcHbJJFfAlM2"
   },
   "outputs": [],
   "source": [
    "data_sel.head()                                                                    # Look at how tokenized data looks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ9BluR-87Dn"
   },
   "source": [
    "- tokenized data sample looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWZwuXaC-4qy"
   },
   "outputs": [],
   "source": [
    "# preparing custom stoprwords list for better result from the analysis\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, so not removing them from original data.\n",
    "\n",
    "stopwords = list(set(stopwords) - set(customlist))                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nnl6yvEQaBmp"
   },
   "outputs": [],
   "source": [
    "#  using Multilingual Wordnet Data from OMW with newer Wordnet versions\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZjCxefg7Et3"
   },
   "outputs": [],
   "source": [
    "# define the word lemmatizer, lowercase conversion, punctuation and stopwords removal functions \n",
    "# apply on the sentiment text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "data_sel['text'] = data_sel.apply(lambda row: normalize(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBJ5s4dk-y1Y"
   },
   "outputs": [],
   "source": [
    "# check sample data\n",
    "data_sel.head(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzieB28wW9SC"
   },
   "outputs": [],
   "source": [
    "#data_sel['text'] = data_sel.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) # Tokenization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_jq7v6pDGV9"
   },
   "source": [
    "- data looks clean after the pre-processing steps are executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peEfVBXZmycA"
   },
   "source": [
    "### **Building the model based on CountVectorizer and Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JqH_4JWOurH"
   },
   "outputs": [],
   "source": [
    "# Vectorization (Convert text data to numbers).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vec = CountVectorizer(max_features=3000)                # Keep only 3000 features as number of features will increase the processing time.\n",
    "data_features = bow_vec.fit_transform(data_sel['text'])\n",
    "data_features = data_features.toarray()                        # Convert the data features to array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-xSq6Y-YN3f"
   },
   "outputs": [],
   "source": [
    "# check the data features shape\n",
    "data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMSVHPhRDh6z"
   },
   "source": [
    "- we have same number of rows as earlier (14,640) but now having 3000 feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcwGQ64UeyTY"
   },
   "outputs": [],
   "source": [
    "#  double check the data types for each field\n",
    "data_sel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjoVkvttgmj4"
   },
   "outputs": [],
   "source": [
    "# convert object columns to category columns\n",
    "data_sel[data_sel.select_dtypes(['object']).columns] = data_sel.select_dtypes(['object']).apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt_rjzW51fx8"
   },
   "outputs": [],
   "source": [
    "labels = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuBWTlx_ZlRo"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtAUjKkZKaoB"
   },
   "outputs": [],
   "source": [
    "# Convert labels from names to one hot vectors.\n",
    "# Labelbinarizer works similar to onehotencoder \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "enc = LabelBinarizer()\n",
    "y_train_encoded = enc.fit_transform(y_train)\n",
    "y_test_encoded=enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrRKfuzXXyPS"
   },
   "outputs": [],
   "source": [
    "# Using Random Forest to build model for the classification of reviews.\n",
    "# Also calculating the cross validation score.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=20, n_jobs=4)\n",
    "\n",
    "forest = forest.fit(X_train, y_train_encoded)\n",
    "print(forest)\n",
    "\n",
    "print(np.mean(cross_val_score(forest, data_features, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3CiPnIOs1R2"
   },
   "source": [
    "### **Optimizing the parameter: Number of trees in the random forest model(n_estimators)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8gZcQLHybbN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Finding optimal number of base learners using k-fold CV ->\n",
    "base_ln = [x for x in range(1, 25)]\n",
    "base_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72VAWWpkyfsi"
   },
   "outputs": [],
   "source": [
    "# K-Fold Cross - validation .\n",
    "cv_scores = []\n",
    "for b in base_ln:\n",
    "    clf = RandomForestClassifier(n_estimators = b)\n",
    "    scores = cross_val_score(clf, X_train, y_train_encoded, cv = 5, scoring = 'accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQWqLot5ygtx"
   },
   "outputs": [],
   "source": [
    "# plotting the error as k increases\n",
    "error = [1 - x for x in cv_scores]                                 #error corresponds to each nu of estimator\n",
    "optimal_learners = base_ln[error.index(min(error))]                #Selection of optimal nu of n_estimator corresponds to minimum error.\n",
    "plt.plot(base_ln, error)                                           #Plot between each nu of estimator and misclassification error\n",
    "xy = (optimal_learners, min(error))\n",
    "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
    "plt.xlabel(\"Number of base learners\")\n",
    "plt.ylabel(\"Misclassification Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlIc9FNFynzp"
   },
   "outputs": [],
   "source": [
    "# Training the best model and calculating accuracy on test data .\n",
    "clf = RandomForestClassifier(n_estimators = optimal_learners)\n",
    "clf.fit(X_train, y_train_encoded)\n",
    "clf.score(X_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edIEmTKz_0ZX"
   },
   "outputs": [],
   "source": [
    "result =  clf.predict(X_test)                  #saving the prediction on test data as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42Y9eabIQ9D8"
   },
   "outputs": [],
   "source": [
    "# Obtaining the categorical values from y_test_encoded and y_pred\n",
    "y_pred_arg=np.argmax(result,axis=1)\n",
    "y_test_arg=np.argmax(y_test_encoded,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdVeV6qRMM1n"
   },
   "outputs": [],
   "source": [
    "# Build the Confusion matirx to get an idea of how the distribution of the prediction is, among all the classes.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test_arg,y_pred_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZQBZq8RTnjh"
   },
   "outputs": [],
   "source": [
    "#Print and plot Confusion matirx \n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    linewidths=.4,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax\n",
    ")\n",
    "# Setting the labels to both the axes\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n",
    "ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_GDPA53zCFu"
   },
   "source": [
    "- true negative sentiment cases are identified with high accuracy\n",
    "- false neutral count is high\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJSADucFKwYQ"
   },
   "source": [
    "### **Word Cloud of top 40 important features from the CountVectorizer + Random Forest based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo1aEFQmH7iX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_features = bow_vec.get_feature_names()              #Instantiate the feature from the vectorizer\n",
    "top_features=''                                            # Addition of top 40 feature into top_feature after training the model\n",
    "feat=clf.feature_importances_\n",
    "features=np.argsort(feat)[::-1]\n",
    "for i in features[0:40]:\n",
    "    top_features+=all_features[i]\n",
    "    top_features+=' '\n",
    "    \n",
    "    \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color=\"white\",colormap='viridis',width=2000, \n",
    "                          height=1000).generate(top_features)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
    "plt.title('Top 40 features WordCloud', fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQLLOuG3L-ia"
   },
   "source": [
    "### **Term Frequency(TF) - Inverse Document Frequency(IDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyWqFm4Bu8gX"
   },
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "data_features = vectorizer.fit_transform(data_sel['text'])\n",
    "\n",
    "data_features = data_features.toarray()\n",
    "\n",
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y9nyb8J-zqo"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.3, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIVmAH39UgKh"
   },
   "outputs": [],
   "source": [
    "# Convert labels from names to one hot vectors.\n",
    "# Labelbinarizer works similar to onehotencoder \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "enc = LabelBinarizer()\n",
    "y_train_encoded = enc.fit_transform(y_train)\n",
    "y_test_encoded=enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7223wy58vjJn"
   },
   "outputs": [],
   "source": [
    "# Using Random Forest to build model for the classification of reviews.\n",
    "# Also calculating the cross validation score.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=20, n_jobs=4)\n",
    "\n",
    "forest = forest.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(forest)\n",
    "\n",
    "print(np.mean(cross_val_score(forest, data_features, labels, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HmehSbKTXjo"
   },
   "outputs": [],
   "source": [
    "# K - Fold Cross Validation .\n",
    "cv_scores = []\n",
    "for b in base_ln:\n",
    "    clf = RandomForestClassifier(n_estimators = b)\n",
    "    scores = cross_val_score(clf, X_train, y_train_encoded, cv = 5, scoring = 'accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkskHcSsThph"
   },
   "outputs": [],
   "source": [
    "# plotting the error as k increases\n",
    "error = [1 - x for x in cv_scores]                                              #error corresponds to each nu of estimator\n",
    "optimal_learners = base_ln[error.index(min(error))]                             #Selection of optimal nu of n_estimator corresponds to minimum error.\n",
    "plt.plot(base_ln, error)                                                        #Plot between each nu of estimator and misclassification error\n",
    "xy = (optimal_learners, min(error))\n",
    "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
    "plt.xlabel(\"Number of base learners\")\n",
    "plt.ylabel(\"Misclassification Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLdplPmNTq4K"
   },
   "outputs": [],
   "source": [
    "# Training the best model and calculating error on test data .\n",
    "clf = RandomForestClassifier(n_estimators = optimal_learners)\n",
    "clf.fit(X_train, y_train_encoded)\n",
    "clf.score(X_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbxP0PUHGJvd"
   },
   "outputs": [],
   "source": [
    "result = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POO2IAQ_eKUN"
   },
   "outputs": [],
   "source": [
    "# Obtaining the categorical values from y_test_encoded and y_pred\n",
    "\n",
    "y_pred_arg=np.argmax(result,axis=1)\n",
    "y_test_arg=np.argmax(y_test_encoded,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI8M6uU8ecvr"
   },
   "outputs": [],
   "source": [
    "# Build the Confusion matirx to get an idea of how the distribution of the prediction is, among all the classes.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test_arg,y_pred_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7ozSw-0fTac"
   },
   "outputs": [],
   "source": [
    "#Print and plot Confusion matirx \n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    linewidths=.4,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax\n",
    ")\n",
    "# Setting the labels to both the axes\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n",
    "ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB5EgtLr9ZcO"
   },
   "source": [
    "- slightly higher number of true negative sentiment cases are identified in this model\n",
    "- false neutral count went even higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBcQtl35lFCJ"
   },
   "source": [
    "### **Word Cloud of top 40 important features from the TF IDF + Random Forest based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psHU9BcCWx22"
   },
   "outputs": [],
   "source": [
    "\n",
    "all_features = vectorizer.get_feature_names()                                #Instantiate the feature from the vectorizer\n",
    "Top_features=''                                                              #Addition of top 40 feature into top_feature after training the model\n",
    "feat=clf.feature_importances_\n",
    "features=np.argsort(feat)[::-1]\n",
    "for i in features[0:40]:\n",
    "    Top_features+=all_features[i]\n",
    "    Top_features+=' '\n",
    "    \n",
    "  \n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color=\"Black\",width=1000, \n",
    "                          height=750).generate(Top_features)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.figure(1, figsize=(30, 30), frameon='equal')\n",
    "plt.title('Top 40 features WordCloud', fontsize=30)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwxfFdwhc-9o"
   },
   "source": [
    "### **Summary**:\n",
    "\n",
    "- We used a dataset which has **reviews in text format and their sentiment score as 'postive', 'negative' and 'neutral'.**\n",
    "- The goal was to **build a model for text-classification**.\n",
    "- We **pre-processed the data** using various techniques and libraries.\n",
    "- We **created a Word Cloud plot** based on negative, postive and top 40 features.\n",
    "- The **pre-processed data is converted to numbers (vectorized)**, so that we can feed the data into the model.\n",
    "- We trained the model and optimized the parameter, which **led to an increase the overall accuracy.**\n",
    "- After building the classification model, we **predicted the results for the test data.**\n",
    "- We saw that using the above techniques, our model performed well in perspective of how text classification models perform.\n",
    "- However, **we can still increase the accuracy of our model by increasing the dataset we took into account for the model building** (We've currently only used 14,460 entries) \n",
    "- We can also increase the **max_feature parameter** in the vectorizer. \n",
    "- We can apply **other model tuning and hyperparameter tuning techniques, as well as other pre-processing techniques** to increase the overall accuracy even further.\n",
    "- We tried to use Count Vectorizer and TF-IDF. Both are giving similar results. However, more true negative cases are identified in case of TF-IDF.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter US Airline Sentiment - Project.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
